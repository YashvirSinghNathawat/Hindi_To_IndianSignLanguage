{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "682564fe-38e4-4f61-9222-6e2eb8e9122d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS TAGS : (PSP)Postposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "838562f8-d416-4c8d-9988-0da61144732f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "import nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83a79957-b38b-4b7a-af8d-b84adeca1ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df11114d-3534-44d1-9ec8-e2288639e8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_dataset.txt\", 'r',encoding='utf8') as file:\n",
    "            contents = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3f765ee-ef87-4adc-afe0-859fe9133bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'only subject and verb': ['राम को स्कूल जाना है', 'मैं खुश हूं।', 'वह गाना गाता है।', 'तुम समय पर आओ।', 'वह चाय पीता है।', 'मेरा दोस्त बच्चों के साथ खेलता है।', 'आपका बेटा पढ़ाई कर रहा है।', 'सोनिया बहुत अच्छी है।', 'मेरे पास एक कुत्ता है।', 'तुम्हारा भाई टेनिस खेलता है।', 'राधा बाजार जाती है।', 'बच्चे पार्क में खेलते हैं।', 'उसका दोस्त उसकी मदद करता है।', 'वह खिड़की साफ़ करता है।', 'सोनल कुछ पढ़ती है।', 'तुम्हारी बहन संगीत सुनती है।', 'मेरी बेटी खाना बनाती है।', 'वह फ़िल्म देखता है।', 'आपका दादा स्कूल जाते हैं।', 'रोहन बहुत खुश है।', 'मेरे दोस्त रेस में भाग लेते हैं।']}\n"
     ]
    }
   ],
   "source": [
    "def create_sentence_dict(text):\n",
    "    \"\"\"Convert text content into a dictionary with categories as keys and sentences as values.\"\"\"\n",
    "    sentence_dict = {}\n",
    "    categories = text.split(\"\\n\\n\")\n",
    "    i = 0\n",
    "    length = len(categories)\n",
    "    \n",
    "    while i < length:\n",
    "        #print(categories[i],categories[i+1].splitlines())\n",
    "        sentences_list = []\n",
    "        for sentence in categories[i+1].splitlines():\n",
    "            if len(sentence)==0:\n",
    "                pass\n",
    "            else:\n",
    "                sentences_list.append(sentence)\n",
    "        sentence_dict[categories[i]] = sentences_list\n",
    "        i+=2\n",
    "        break\n",
    "    # for category in categories:\n",
    "    #     lines = category.split(\"\\n\")\n",
    "    #     key = lines[0].strip()\n",
    "    #     sentences = [line.strip() for line in lines[1:]]\n",
    "    #     sentence_dict[key] = sentences\n",
    "        \n",
    "    return sentence_dict\n",
    "# Create the dictionary\n",
    "sentence_dict = create_sentence_dict(contents)\n",
    "print(sentence_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aea6b3b4-3730-4745-9f15-a0d62dca1655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 02:50:23 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c0fb75a1e9c4d36835950444af8ef02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 02:50:23 INFO: Downloaded file to C:\\Users\\Lenovo\\stanza_resources\\resources.json\n",
      "2024-04-21 02:50:24 INFO: Loading these models for language: hi (Hindi):\n",
      "=============================\n",
      "| Processor | Package       |\n",
      "-----------------------------\n",
      "| tokenize  | hdtb          |\n",
      "| pos       | hdtb_charlm   |\n",
      "| lemma     | hdtb_nocharlm |\n",
      "| depparse  | hdtb_charlm   |\n",
      "=============================\n",
      "\n",
      "2024-04-21 02:50:24 INFO: Using device: cpu\n",
      "2024-04-21 02:50:24 INFO: Loading: tokenize\n",
      "2024-04-21 02:50:25 INFO: Loading: pos\n",
      "2024-04-21 02:50:25 INFO: Loading: lemma\n",
      "2024-04-21 02:50:25 INFO: Loading: depparse\n",
      "2024-04-21 02:50:25 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline('hi', processors='tokenize,lemma,pos,depparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1c9a602-d4ad-4cb3-81fc-60ed874caba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PSP', 'NN', 'VM', 'NNP', 'VAUX', 'SYM', 'JJ', 'NNPC', 'PRP', 'CC', 'NNC', 'QC', 'NST', 'DEM', 'RP', 'QF', 'NEG', 'RB', 'QCC', 'QO', 'INTF', 'JJC', 'WQ', 'RDP', 'UNK', 'PRPC', 'NSTC', 'RBC', 'QFC', 'CCC', 'INJ']\n"
     ]
    }
   ],
   "source": [
    "# Known POS Tags\n",
    "print(nlp.processors['pos'].get_known_xpos())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33f6665f-0f38-41bf-9027-df63b3347731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"text\": \"वह\",\n",
      "      \"lemma\": \"वह\",\n",
      "      \"upos\": \"PRON\",\n",
      "      \"xpos\": \"PRP\",\n",
      "      \"feats\": \"Case=Nom|Number=Sing|Person=3|PronType=Prs\",\n",
      "      \"head\": 3,\n",
      "      \"deprel\": \"nsubj\",\n",
      "      \"start_char\": 0,\n",
      "      \"end_char\": 2\n",
      "    },\n",
      "    {\n",
      "      \"id\": 2,\n",
      "      \"text\": \"अधिक\",\n",
      "      \"lemma\": \"अधिक\",\n",
      "      \"upos\": \"DET\",\n",
      "      \"xpos\": \"QF\",\n",
      "      \"feats\": \"PronType=Ind\",\n",
      "      \"head\": 3,\n",
      "      \"deprel\": \"xcomp\",\n",
      "      \"start_char\": 3,\n",
      "      \"end_char\": 7\n",
      "    },\n",
      "    {\n",
      "      \"id\": 3,\n",
      "      \"text\": \"सोती\",\n",
      "      \"lemma\": \"सो\",\n",
      "      \"upos\": \"VERB\",\n",
      "      \"xpos\": \"VM\",\n",
      "      \"feats\": \"Aspect=Imp|Gender=Fem|Number=Sing|Person=3|VerbForm=Part|Voice=Act\",\n",
      "      \"head\": 0,\n",
      "      \"deprel\": \"root\",\n",
      "      \"start_char\": 8,\n",
      "      \"end_char\": 12\n",
      "    },\n",
      "    {\n",
      "      \"id\": 4,\n",
      "      \"text\": \"है\",\n",
      "      \"lemma\": \"है\",\n",
      "      \"upos\": \"AUX\",\n",
      "      \"xpos\": \"VAUX\",\n",
      "      \"feats\": \"Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\",\n",
      "      \"head\": 3,\n",
      "      \"deprel\": \"aux\",\n",
      "      \"start_char\": 13,\n",
      "      \"end_char\": 15,\n",
      "      \"misc\": \"SpaceAfter=No\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 5,\n",
      "      \"text\": \"।\",\n",
      "      \"lemma\": \"।\",\n",
      "      \"upos\": \"PUNCT\",\n",
      "      \"xpos\": \"SYM\",\n",
      "      \"head\": 3,\n",
      "      \"deprel\": \"punct\",\n",
      "      \"start_char\": 15,\n",
      "      \"end_char\": 16,\n",
      "      \"misc\": \"SpaceAfter=No\"\n",
      "    }\n",
      "  ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "#sentence = sentence_dict['only subject and verb'][20]\n",
    "sentence = 'वह अधिक सोती है।'\n",
    "sentence_list = sentence.split()\n",
    "doc = nlp(sentence)\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf5687e-97ff-4577-9fd8-fbc625a069ce",
   "metadata": {},
   "source": [
    "# Sentences\n",
    "उसने खुशी से कहा\n",
    "\n",
    "1. Noun Verb\n",
    "\n",
    "2. Assumption:  Subject always comes before object\n",
    "    राम ने श्याम को कहा\n",
    "    Subject Object Verb\n",
    "\n",
    "3. Verb Auxiliary Verb\n",
    "\n",
    "4. Verb Adverb\n",
    "\n",
    "5. Noun Adjective\n",
    "\n",
    "-----------------------------------------------------\n",
    "#### Subjective S_Adjective Object O_Adjective Verb Adverb\n",
    "-----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cad33a4b-af56-4971-8c93-f90ea08ace67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': 'वह', 'xpos': 'PRP', 'id': 1, 'lemma': 'वह', 'deprel': 'nsubj', 'head': 3}, {'text': 'अधिक', 'xpos': 'QF', 'id': 2, 'lemma': 'अधिक', 'deprel': 'xcomp', 'head': 3}, {'text': 'सोती', 'xpos': 'VM', 'id': 3, 'lemma': 'सो', 'deprel': 'root', 'head': 0}, {'text': 'है', 'xpos': 'VAUX', 'id': 4, 'lemma': 'है', 'deprel': 'aux', 'head': 3}, {'text': '।', 'xpos': 'SYM', 'id': 5, 'lemma': '।', 'deprel': 'punct', 'head': 3}]\n"
     ]
    }
   ],
   "source": [
    "# Extract tags for hindi sentences\n",
    "word_tag = []\n",
    "for sent in doc.sentences:\n",
    "    for word in sent.words:\n",
    "        word_dict = {\n",
    "            \"text\": word.text,\n",
    "            \"xpos\": word.xpos,\n",
    "            \"id\": word.id,\n",
    "            \"lemma\": word.lemma,\n",
    "            \"deprel\": word.deprel,\n",
    "            \"head\": word.head\n",
    "        }\n",
    "        word_tag.append(word_dict)\n",
    "print(word_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "858e6bc7-2626-4339-8e2b-0fe6a2145902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: {'text': 'वह', 'xpos': 'PRP', 'id': 1, 'lemma': 'वह', 'deprel': 'nsubj', 'head': 3}, 2: {'text': 'अधिक', 'xpos': 'QF', 'id': 2, 'lemma': 'अधिक', 'deprel': 'xcomp', 'head': 3}, 3: {'text': 'सोती', 'xpos': 'VM', 'id': 3, 'lemma': 'सो', 'deprel': 'root', 'head': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Removing Unwanted Tags - VAUX CC SYM\n",
    "unwanted_tags = ['VAUX','CC','SYM','PSP']\n",
    "word_tag_cleaned = {}\n",
    "for rel_dict in word_tag:\n",
    "    if rel_dict['xpos'] not in unwanted_tags:\n",
    "        word_tag_cleaned[rel_dict['id']] =  rel_dict\n",
    "print(word_tag_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ee777d6-4bcf-42a6-a562-40e15e0613ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {'text': 'वह', 'xpos': 'PRP', 'id': 1, 'lemma': 'वह', 'deprel': 'nsubj', 'head': 3}\n",
      "2 {'text': 'अधिक', 'xpos': 'QF', 'id': 2, 'lemma': 'अधिक', 'deprel': 'xcomp', 'head': 3}\n",
      "3 {'text': 'सोती', 'xpos': 'VM', 'id': 3, 'lemma': 'सो', 'deprel': 'root', 'head': 0}\n"
     ]
    }
   ],
   "source": [
    "for key,value in word_tag_cleaned.items():\n",
    "    print(key,value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659148b0-9d55-4978-9d21-f2f8fd04b7d3",
   "metadata": {},
   "source": [
    "## Converting TO SIGN GRAMMER\n",
    "#### Arrange in Order : Pronoun Subjective S_Adjective Object O_Adjective -> Noun Adjective Verb Adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f240f2c0-ff10-4bbc-a62b-fe1bd430beb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_dict_order(sample_dict,row_1,row_2):\n",
    "    list_form = [(key,value) for key,value in sample_dict.items()]\n",
    "    index_1 = None     # index_1 \n",
    "    index_2 = None\n",
    "    for i,content in enumerate(list_form):\n",
    "        if content[0] == row_1:\n",
    "            index_1 = i\n",
    "        elif content[0] == row_2:\n",
    "            index_2 = i\n",
    "    entry_1 = list_form.pop(index_1)\n",
    "    print(index_1,index_2,entry_1)\n",
    "    list_form.insert(index_2+1,entry_1)\n",
    "\n",
    "    print(list_form)\n",
    "    \n",
    "\n",
    "    \n",
    "    return_dict = {}\n",
    "    for key,value in list_form:\n",
    "        return_dict[key] = value\n",
    "    return return_dict\n",
    "\n",
    "# sample_dict = {1: \"Yashvir\",\n",
    "#                2: \"Singh\",\n",
    "#                3: \"Nathawat\",\n",
    "#                4: \"ABC\",\n",
    "#               5: \"EFC\"}\n",
    "# output = change_dict_order(sample_dict,2,4)\n",
    "# for key,value in output.items():\n",
    "#     print(key,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "631b7b33-bb92-468d-b1b9-e054fc1a3906",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m         subject_index \u001b[38;5;241m=\u001b[39m cnt\n\u001b[0;32m     16\u001b[0m     cnt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 17\u001b[0m word_sign_form \u001b[38;5;241m=\u001b[39m \u001b[43mchange_dict_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword_sign_form\u001b[49m\u001b[43m,\u001b[49m\u001b[43mobject_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43msubject_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 10\u001b[0m, in \u001b[0;36mchange_dict_order\u001b[1;34m(sample_dict, row_1, row_2)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m content[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m row_2:\n\u001b[0;32m      9\u001b[0m         index_2 \u001b[38;5;241m=\u001b[39m i\n\u001b[1;32m---> 10\u001b[0m entry_1 \u001b[38;5;241m=\u001b[39m \u001b[43mlist_form\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(index_1,index_2,entry_1)\n\u001b[0;32m     12\u001b[0m list_form\u001b[38;5;241m.\u001b[39minsert(index_2\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,entry_1)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "# Not Done\n",
    "word_sign_form = word_tag_cleaned.copy()\n",
    "# Format Subject then Object\n",
    "subject_id = None\n",
    "object_id = None\n",
    "subject_index = None \n",
    "object_index = None\n",
    "cnt = 0\n",
    "for key,value in word_tag_cleaned.items():\n",
    "    if value['deprel'] in ['obj','obl']:\n",
    "        object_id = key\n",
    "        object_index = cnt\n",
    "    elif value['deprel']=='nsubj':\n",
    "        subject_id = key\n",
    "        subject_index = cnt\n",
    "    cnt+=1\n",
    "word_sign_form = change_dict_order(word_sign_form,object_id,subject_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee030097-493a-47f8-81e7-a09343aa597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in word_sign_form.items():\n",
    "    print(key,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12b3bc49-fd8e-4d95-a55c-2730263f557d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: {'text': 'वह', 'xpos': 'PRP', 'id': 1, 'lemma': 'वह', 'deprel': 'nsubj', 'head': 3}, 2: {'text': 'अधिक', 'xpos': 'QF', 'id': 2, 'lemma': 'अधिक', 'deprel': 'xcomp', 'head': 3}, 3: {'text': 'सोती', 'xpos': 'VM', 'id': 3, 'lemma': 'सो', 'deprel': 'root', 'head': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Arrange Adjective and Adverb\n",
    "for key,value in word_tag_cleaned.items():    # word_tag_cleaned\n",
    "    if value['xpos'] in ['JJ']:     # Adjective Adverb\n",
    "        # first condition is for when it does not have corresponding noun or verb - मैं खुश हूं।\n",
    "        if value['head']!=0 and word_tag_cleaned[value['head']]['xpos'] in ['NN']:\n",
    "            word_sign_form = change_dict_order(word_sign_form,key,value['head'])\n",
    "    elif value['xpos'] in ['RB']:\n",
    "        if value['head']!=0 and word_tag_cleaned[value['head']]['xpos'] in ['VM','VAUX']:\n",
    "            word_sign_form = change_dict_order(word_sign_form,key,value['head'])\n",
    "print(word_sign_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86d0f059-dd69-49d8-9f1a-01a450617fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange Negative Sentences\n",
    "for key,value in word_sign_form.items():\n",
    "    if value['xpos']=='NEG': \n",
    "        last_key = list(word_sign_form.keys())[-1]\n",
    "        word_sign_form = change_dict_order(word_sign_form,key,last_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ec4a17-c6ad-4d7b-8197-c207e004e128",
   "metadata": {},
   "source": [
    "# StopWord Removal - For Sentences like मैं खुश हूं।\n",
    "#### हूं - comes out to be VM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d6aa9b6-a5f5-40be-b999-51efa9cf8da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read stopwords from file\n",
    "with open('final_stopwords.txt', 'r',encoding='utf8') as file:\n",
    "    # Read the entire contents of the file\n",
    "    stopword_list = file.readlines()\n",
    "stopword_list = [word.strip() for word in stopword_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5a534df-a946-479a-be91-951539f7b6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StopWord Removal\n",
    "stopword_removed_list = {}\n",
    "for key,value in word_sign_form.items():\n",
    "    #print(value['text'] in stopword_list)\n",
    "    if value['text'] not in stopword_list:\n",
    "        stopword_removed_list[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd1c67d0-8a61-4bdf-a55f-220fbaa30476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {'text': 'वह', 'xpos': 'PRP', 'id': 1, 'lemma': 'वह', 'deprel': 'nsubj', 'head': 3}\n",
      "2 {'text': 'अधिक', 'xpos': 'QF', 'id': 2, 'lemma': 'अधिक', 'deprel': 'xcomp', 'head': 3}\n",
      "3 {'text': 'सोती', 'xpos': 'VM', 'id': 3, 'lemma': 'सो', 'deprel': 'root', 'head': 0}\n"
     ]
    }
   ],
   "source": [
    "for key,value in stopword_removed_list.items():\n",
    "    print(key,value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d77531-eae4-44d7-84f9-ec235cab9eda",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "### Need : वह अधिक सोती है।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a191588a-5c09-47f5-800e-012f7f5fcbd5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nlu'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnlu\u001b[39;00m\n\u001b[0;32m      2\u001b[0m nlu\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhi.lemma.hdtb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mI love Spark NLP\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nlu'"
     ]
    }
   ],
   "source": [
    "import nlu\n",
    "nlu.load(\"hi.lemma.hdtb\").predict(\"\"\"I love Spark NLP\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e686ca02-4b16-4d06-b580-3bdc2297492a",
   "metadata": {},
   "source": [
    "# Mapping xpos to POS Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c99404b-f6b5-4efb-9a1c-3ccd3424e552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping xpos to POS tags\n",
    "xpos_to_pos = {\n",
    "    'NNP': 'pnoun',\n",
    "    'VM': 'verb',\n",
    "    'VAUX': 'verb',\n",
    "    'JJ': 'adjective',\n",
    "    'RB': 'adverb',\n",
    "    'PRP' : 'pronoun',\n",
    "    'NEG' : 'negative',\n",
    "    'NN' : 'noun',\n",
    "    'RDP' : 'adverb',\n",
    "    'QF': 'adjective'            # 'अधिक'\n",
    "    # Add more mappings as needed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d4298ba-cb9d-4d2c-95ea-730d5a399596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('वह', 'pronoun'), ('अधिक', 'adjective'), ('सोती', 'verb')]\n"
     ]
    }
   ],
   "source": [
    "# Extract Words from Parser and corresponding tag\n",
    "sign_words_list = []\n",
    "for key,value in stopword_removed_list.items():\n",
    "    sign_words_list.append((value['text'],xpos_to_pos[value['xpos']]))\n",
    "print(sign_words_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b50dac-bc18-44eb-98d4-7aed38eba4f5",
   "metadata": {},
   "source": [
    "## Synonym Substitution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0236114d-93e8-4a6b-8705-9619b997c142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ ISL Dictionary\n",
    "import ast\n",
    "# Open the file in read mode\n",
    "with open('./isl_dict.txt', 'r',encoding='utf8') as file:\n",
    "    # Read the entire contents of the file\n",
    "    isl_dict = ast.literal_eval(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2a794e3-b0b0-444c-87cd-d3f9cf4ef99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dictionary with lowercase keys\n",
    "isl_dict = {key.lower(): value for key, value in isl_dict.items()}\n",
    "isl_dict['school'] = 'स्कूल'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6ea41ba-a207-4733-81ca-e1500eaa2699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Translator\n",
    "from googletrans import Translator\n",
    "# Create a Translator object\n",
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1956169d-5b79-4e73-a3fa-48e1d9f24ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21:02:50:37,914 INFO     [iwn.py:43] Loading hindi language synsets...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_load_synset', '_load_synset_file', '_load_synset_relations', '_relation_list', '_synset_df', '_synset_idx_map', '_synset_relations_dict', '_update_synset_idx_map', 'all_synsets', 'all_words', 'synset_relation', 'synsets']\n",
      "[Synset('आम.noun.3462')]\n",
      "<bound method IndoWordNet.all_synsets of <pyiwn.iwn.IndoWordNet object at 0x00000183E8739E10>>\n"
     ]
    }
   ],
   "source": [
    "# Synonym Substitution\n",
    "import pyiwn\n",
    "from nltk.corpus import wordnet as wn\n",
    "iwn = pyiwn.IndoWordNet() \n",
    "print(dir(iwn))\n",
    "print(iwn.synsets('आम्र'))\n",
    "print(iwn.all_synsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62945f12-845e-4762-b1b1-6bfd8a9b698e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he\n",
      "An error occurred: 'वह'\n",
      "more\n",
      "fantasy\n",
      "An error occurred: 'सोती'\n",
      "[('वह', 'pronoun', 'he'), ('अधिक', 'adjective', 'more'), ('सोती', 'verb', '#')]\n"
     ]
    }
   ],
   "source": [
    "synonym_substituted_list = []\n",
    "temp_list = [('आकलन', 'noun')]\n",
    "for word,pos_tag in sign_words_list:\n",
    "\n",
    "    # Translate the Hindi sentence to English\n",
    "    english_word = translator.translate(word, src='hi', dest='en').text.lower()\n",
    "    print(english_word)\n",
    "    \n",
    "    if pos_tag == 'pnoun':\n",
    "        synonym_substituted_list.append((word,pos_tag,english_word))\n",
    "        continue\n",
    "    #print(word,pos_tag)\n",
    "    \n",
    "    # Case 1 : Check hindi word in isl_dict\n",
    "    if word in list(isl_dict.values()):\n",
    "        synonym_substituted_list.append((word,pos_tag,english_word))\n",
    "        continue\n",
    "    all_hindi_synsets = []\n",
    "    # Case 2 : Check synonym of hindi_word in isl_dict\n",
    "    try:\n",
    "        all_hindi_synsets = iwn.synsets(word)\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "        pass\n",
    "    flag = False\n",
    "    for synset in all_hindi_synsets:\n",
    "        if synset._head_word in list(isl_dict.values()):\n",
    "            corresponding_keys = [key for key, value in isl_dict.items() if value == synset._head_word]\n",
    "            synonym_substituted_list.append((word,pos_tag,corresponding_keys[0]))\n",
    "            flag = True\n",
    "            continue\n",
    "    if flag == True:\n",
    "        continue\n",
    "                \n",
    "    # Case 3 : Check english word in isl_dict\n",
    "    if english_word in list(isl_dict.keys()):\n",
    "        synonym_substituted_list.append((word,pos_tag,english_word))\n",
    "        continue\n",
    "\n",
    "    # Case 4 : Check syno of english word in isl_dict\n",
    "    all_english_synsets = wn.synonyms(english_word)\n",
    "    #print(all_english_synsets)\n",
    "    all_english_synsets_flatten = []\n",
    "    for row in all_english_synsets:\n",
    "        all_english_synsets_flatten.extend(row)\n",
    "    flag = False\n",
    "    for synset in all_english_synsets_flatten:\n",
    "        if synset.lower() in list(isl_dict.keys()):\n",
    "            flag = True\n",
    "            # print('Yes Present')\n",
    "            synonym_substituted_list.append((word,pos_tag,synset.lower()))\n",
    "            break\n",
    "    if flag == True:\n",
    "        continue\n",
    "\n",
    "    \n",
    "    # Case 5 : Most Similar Words using WordNet Path Similarity Concept\n",
    "    #for word in list(isl_dict.keys()):\n",
    "\n",
    "    # Case 6  : Nothing Words Go for Finger Spelling\n",
    "    synonym_substituted_list.append((word,pos_tag,'#'))\n",
    "print(synonym_substituted_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a4a7964-5c69-4e45-b9c0-56b502243502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('वह', 'pronoun', 'he'), ('अधिक', 'adjective', 'more'), ('सोती', 'verb', '#')]\n"
     ]
    }
   ],
   "source": [
    "# Final ISL List\n",
    "final_isl_list = synonym_substituted_list.copy()\n",
    "print(final_isl_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a51cf86-f988-4b45-8eeb-1a5c9081f6a8",
   "metadata": {},
   "source": [
    "# Output to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "acf886e5-fd9f-479f-936e-7eaaf4e1675b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"manual_output.txt\", 'a', encoding='utf-8') as file:\n",
    "        isl_list = []\n",
    "        for list_1 in synonym_substituted_list:\n",
    "            isl_list.append(list_1)\n",
    "        file.write(str(sentence_list) + \" - \" + str(isl_list) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1570af-2dcc-4bd0-a476-4f2400368ac5",
   "metadata": {},
   "source": [
    "# Video Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "f32f543b-b8b2-4e3f-8e83-230f9aea59b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reversed dictionary mapping Hindi words to English words\n",
    "isl_hindi_english_dict = {hindi_word: english_word for english_word, hindi_word in isl_dict.items()}\n",
    "#print(isl_hindi_english_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "16cca5fc-cb58-4d8a-9971-6aa20cb54e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "def search_videos(folder_path, final_isl_list):\n",
    "    \"\"\"\n",
    "    Searches for video files named after the provided words in a directory.\n",
    "\n",
    "    Args:\n",
    "        folder_path: Path to the directory containing video files.\n",
    "        words: A list of words to search for (video names).\n",
    "\n",
    "    Returns:\n",
    "        A list of paths to the found video files.\n",
    "    \"\"\"\n",
    "\n",
    "    found_videos = []\n",
    "    # Assuming `words` is a list of tuples like [(or_word1, pos_tag1, isl_word1), (or_word2, pos_tag2, isl_word2), ...]\n",
    "    for or_word, pos_tag, isl_word in final_isl_list:\n",
    "        video_name = f\"{isl_word.capitalize()}.mp4\"\n",
    "        if pos_tag in ['pnoun','#']:  # Alphabets and FingerSpell\n",
    "            for letter in isl_word:\n",
    "                video_name = f\"{letter.capitalize()}.mp4\"\n",
    "                for root, dirs, files in os.walk(folder_path):\n",
    "                    full_path = os.path.join(root, video_name)\n",
    "                    if os.path.isfile(full_path):\n",
    "                        found_videos.append(full_path)\n",
    "        else:\n",
    "            for root, dirs, files in os.walk(folder_path):\n",
    "                full_path = os.path.join(root, video_name)\n",
    "                if os.path.isfile(full_path):\n",
    "                    found_videos.append(full_path)\n",
    "\n",
    "    return found_videos\n",
    "\n",
    "\n",
    "def play_videos(video_paths):\n",
    "    vlc_path = r'C:\\Program Files\\VideoLAN\\VLC\\vlc.exe'  # Path to VLC media player executable\n",
    "    for video_path in video_paths:\n",
    "        subprocess.Popen([vlc_path, '--fullscreen', video_path])\n",
    "        time.sleep(5)  # Delay before playing the next video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "f34f5062-592d-4673-8931-c9b9a8a84383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D:\\\\desktop\\\\project\\\\Linguistic\\\\H\\\\He.mp4', 'D:\\\\desktop\\\\project\\\\Linguistic\\\\P\\\\Place.mp4', 'D:\\\\desktop\\\\project\\\\Linguistic\\\\G\\\\Gradual.mp4', 'D:\\\\desktop\\\\project\\\\Linguistic\\\\A\\\\Arrive.mp4']\n"
     ]
    }
   ],
   "source": [
    "folder_path = 'D:\\desktop\\project\\Linguistic'\n",
    "#print(final_isl_list)\n",
    "video_paths = search_videos(folder_path, final_isl_list)\n",
    "print(video_paths)\n",
    "#play_videos(video_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "072f48c5-87c7-46e7-954c-07a78e698a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video merged_video.mp4.\n",
      "Moviepy - Writing video merged_video.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready merged_video.mp4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['C:\\\\Program Files\\\\VideoLAN\\\\VLC\\\\vlc.exe',...>"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip, concatenate_videoclips\n",
    "vlc_path = r'C:\\Program Files\\VideoLAN\\VLC\\vlc.exe' \n",
    "def merge_videos(video_paths):\n",
    "    clips = [VideoFileClip(path) for path in video_paths]\n",
    "    final_clip = concatenate_videoclips(clips, method=\"compose\")\n",
    "    return final_clip\n",
    "\n",
    "# Merge the videos into a single video\n",
    "merged_clip = merge_videos(video_paths)\n",
    "\n",
    "\n",
    "# Export the merged video to a file\n",
    "merged_clip.write_videofile(\"merged_video.mp4\")\n",
    "\n",
    "# Play the merged video\n",
    "subprocess.Popen([vlc_path, '--fullscreen', 'merged_video.mp4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "d1586e6d-2cb3-4204-af50-d0cebb2e21be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('बहुत.adjective.2403'), Synset('बचा.adjective.7074'), Synset('ऊपर.adverb.11702'), Synset('बहुत.adverb.13340'), Synset('अधिक.noun.23463'), Synset('अधिकांशीय.adjective.28616'), Synset('अधिक.adjective.37939')]\n"
     ]
    }
   ],
   "source": [
    "print(iwn.synsets('अधिक'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "1b3ad54f-6a5c-48d1-acf2-4d6a78aab5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fantasy\n"
     ]
    }
   ],
   "source": [
    "print(translator.translate('सोती', src='hi', dest='en').text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cddaa1-2441-4d53-a54d-a2e83cf582c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
