{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb02d10e-b557-445b-9992-26621bcc0e06",
   "metadata": {},
   "source": [
    "# CS 689A Project - Hindi To Indian Sign Language: Rule-Based Translation System\n",
    "### Done by\n",
    "1. Yashvir Singh Nathawat(231110059)\n",
    "2. Karthik Jain (231110023)\n",
    "3. Aditya Katare (231110005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "838562f8-d416-4c8d-9988-0da61144732f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\Desktop\\Projects\\Hindi_To_IndianSignLanguage\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import statements\n",
    "import stanza\n",
    "import nlp\n",
    "import pandas as pd\n",
    "import ast\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb3fcb2-4549-4e7d-ba44-5e4683fec551",
   "metadata": {},
   "source": [
    "# Input the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50408616-55ff-4321-803c-03e5ec85dba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the hindi sentence :  मेरा भारत महान है।\n"
     ]
    }
   ],
   "source": [
    "sentence = input('Enter the hindi sentence : ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce051582-db9b-4947-81c1-7623f666f2cb",
   "metadata": {},
   "source": [
    "# POS Tagging and Dependency Parsing - STANZA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aea6b3b4-3730-4745-9f15-a0d62dca1655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-03 19:21:40 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 7.82MB/s]                    \n",
      "2024-11-03 19:21:41 INFO: Downloaded file to C:\\Users\\Lenovo\\stanza_resources\\resources.json\n",
      "2024-11-03 19:21:42 INFO: Loading these models for language: hi (Hindi):\n",
      "=============================\n",
      "| Processor | Package       |\n",
      "-----------------------------\n",
      "| tokenize  | hdtb          |\n",
      "| pos       | hdtb_charlm   |\n",
      "| lemma     | hdtb_nocharlm |\n",
      "| depparse  | hdtb_charlm   |\n",
      "=============================\n",
      "\n",
      "2024-11-03 19:21:42 INFO: Using device: cpu\n",
      "2024-11-03 19:21:42 INFO: Loading: tokenize\n",
      "C:\\Users\\Lenovo\\Desktop\\Projects\\Hindi_To_IndianSignLanguage\\venv\\Lib\\site-packages\\stanza\\models\\tokenization\\trainer.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-03 19:21:43 INFO: Loading: pos\n",
      "C:\\Users\\Lenovo\\Desktop\\Projects\\Hindi_To_IndianSignLanguage\\venv\\Lib\\site-packages\\stanza\\models\\pos\\trainer.py:139: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "C:\\Users\\Lenovo\\Desktop\\Projects\\Hindi_To_IndianSignLanguage\\venv\\Lib\\site-packages\\stanza\\models\\common\\pretrain.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(self.filename, lambda storage, loc: storage)\n",
      "C:\\Users\\Lenovo\\Desktop\\Projects\\Hindi_To_IndianSignLanguage\\venv\\Lib\\site-packages\\stanza\\models\\common\\char_model.py:271: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-03 19:21:44 INFO: Loading: lemma\n",
      "C:\\Users\\Lenovo\\Desktop\\Projects\\Hindi_To_IndianSignLanguage\\venv\\Lib\\site-packages\\stanza\\models\\lemma\\trainer.py:239: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-03 19:21:44 INFO: Loading: depparse\n",
      "C:\\Users\\Lenovo\\Desktop\\Projects\\Hindi_To_IndianSignLanguage\\venv\\Lib\\site-packages\\stanza\\models\\depparse\\trainer.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-03 19:21:44 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PSP', 'NN', 'VM', 'NNP', 'SYM', 'VAUX', 'JJ', 'NNPC', 'PRP', 'CC', 'NNC', 'QC', 'NST', 'DEM', 'RP', 'QF', 'NEG', 'RB', 'QCC', 'QO', 'INTF', 'JJC', 'WQ', 'RDP', 'UNK', 'PRPC', 'NSTC', 'RBC', 'QFC', 'CCC', 'INJ']\n"
     ]
    }
   ],
   "source": [
    "# Import Stanza Hindi Pipeline\n",
    "nlp = stanza.Pipeline('hi', processors='tokenize,lemma,pos,depparse')\n",
    "# Known POS Tags\n",
    "print(nlp.processors['pos'].get_known_xpos())\n",
    "doc = nlp(sentence)\n",
    "#print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33f6665f-0f38-41bf-9027-df63b3347731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>upos</th>\n",
       "      <th>xpos</th>\n",
       "      <th>head</th>\n",
       "      <th>deprel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>मेरा</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRP</td>\n",
       "      <td>2</td>\n",
       "      <td>nmod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>भारत</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>3</td>\n",
       "      <td>nsubj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>महान</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>JJ</td>\n",
       "      <td>0</td>\n",
       "      <td>root</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>है</td>\n",
       "      <td>AUX</td>\n",
       "      <td>VM</td>\n",
       "      <td>3</td>\n",
       "      <td>cop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>।</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>SYM</td>\n",
       "      <td>3</td>\n",
       "      <td>punct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   text   upos xpos  head deprel\n",
       "0  मेरा   PRON  PRP     2   nmod\n",
       "1  भारत  PROPN  NNP     3  nsubj\n",
       "2  महान    ADJ   JJ     0   root\n",
       "3    है    AUX   VM     3    cop\n",
       "4     ।  PUNCT  SYM     3  punct"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract relevant fields from the data\n",
    "df_data = [(token['text'], token['upos'], token['xpos'], token['head'], token['deprel']) for token in doc.to_dict()[0]]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(df_data, columns=['text', 'upos', 'xpos', 'head', 'deprel'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf5687e-97ff-4577-9fd8-fbc625a069ce",
   "metadata": {},
   "source": [
    "# Extracting Tags for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cad33a4b-af56-4971-8c93-f90ea08ace67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tags for hindi sentences\n",
    "word_tag = []\n",
    "for sent in doc.sentences:\n",
    "    for word in sent.words:\n",
    "        word_dict = {\n",
    "            \"text\": word.text,\n",
    "            \"xpos\": word.xpos,\n",
    "            \"id\": word.id,\n",
    "            \"lemma\": word.lemma,\n",
    "            \"deprel\": word.deprel,\n",
    "            \"head\": word.head\n",
    "        }\n",
    "        word_tag.append(word_dict)\n",
    "#print(word_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa79477-752b-4a55-8d60-81ce4728707c",
   "metadata": {},
   "source": [
    "# Removing Unwanted Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "858e6bc7-2626-4339-8e2b-0fe6a2145902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Unwanted Tags - VAUX CC SYM\n",
    "unwanted_tags = ['VAUX','CC','SYM','PSP']\n",
    "word_tag_cleaned = {}\n",
    "for rel_dict in word_tag:\n",
    "    if rel_dict['xpos'] not in unwanted_tags:\n",
    "        word_tag_cleaned[rel_dict['id']] =  rel_dict\n",
    "#print(word_tag_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ee777d6-4bcf-42a6-a562-40e15e0613ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {'text': 'मेरा', 'xpos': 'PRP', 'id': 1, 'lemma': 'मैं', 'deprel': 'nmod', 'head': 2}\n",
      "2 {'text': 'भारत', 'xpos': 'NNP', 'id': 2, 'lemma': 'भारत', 'deprel': 'nsubj', 'head': 3}\n",
      "3 {'text': 'महान', 'xpos': 'JJ', 'id': 3, 'lemma': 'महान', 'deprel': 'root', 'head': 0}\n",
      "4 {'text': 'है', 'xpos': 'VM', 'id': 4, 'lemma': 'है', 'deprel': 'cop', 'head': 3}\n"
     ]
    }
   ],
   "source": [
    "for key,value in word_tag_cleaned.items():\n",
    "    print(key,value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659148b0-9d55-4978-9d21-f2f8fd04b7d3",
   "metadata": {},
   "source": [
    "# Grammer Rules based Reordering\n",
    "\n",
    "1. Noun Verb\n",
    "\n",
    "2. Assumption:  Subject always comes before object\n",
    "    राम ने श्याम को कहा\n",
    "    Subject Object Verb\n",
    "\n",
    "3. Verb Auxiliary Verb\n",
    "\n",
    "4. Verb Adverb\n",
    "\n",
    "5. Noun Adjective\n",
    "\n",
    "-----------------------------------------------------\n",
    "#### Subjective S_Adjective Object O_Adjective Verb Adverb\n",
    "-----------------------------------------------------\n",
    "#### Arrange in Order : Pronoun Subjective S_Adjective Object O_Adjective -> Noun Adjective Verb Adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f240f2c0-ff10-4bbc-a62b-fe1bd430beb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_dict_order(sample_dict,row_1,row_2):\n",
    "    if row_1 == row_2:\n",
    "        return sample_dict\n",
    "    list_form = [(key,value) for key,value in sample_dict.items()]\n",
    "    index_1 = None     # index_1 \n",
    "    index_2 = None\n",
    "    for i,content in enumerate(list_form):\n",
    "        if content[0] == row_1:\n",
    "            index_1 = i\n",
    "        elif content[0] == row_2:\n",
    "            index_2 = i\n",
    "    entry_1 = list_form.pop(index_1)\n",
    "    list_form.insert(index_2,entry_1)    \n",
    "    return_dict = {}\n",
    "    for key,value in list_form:\n",
    "        return_dict[key] = value\n",
    "    return return_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b7e690-9dbb-4bf2-820b-71714d96b959",
   "metadata": {},
   "source": [
    "# Format Subject and Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "631b7b33-bb92-468d-b1b9-e054fc1a3906",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sign_form = word_tag_cleaned.copy()\n",
    "# Format Subject then Object\n",
    "subject_id = None\n",
    "object_id = None\n",
    "subject_index = None \n",
    "object_index = None\n",
    "cnt = 0\n",
    "for key,value in word_tag_cleaned.items():\n",
    "    if value['deprel'] in ['obj','obl']:\n",
    "        object_id = key\n",
    "        object_index = cnt\n",
    "    elif value['deprel']=='nsubj':\n",
    "        subject_id = key\n",
    "        subject_index = cnt\n",
    "    cnt+=1\n",
    "#print(subject_id,object_id)\n",
    "if subject_id!=None and object_id!=None and subject_id>object_id:\n",
    "    word_sign_form = change_dict_order(word_sign_form,object_id,subject_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee030097-493a-47f8-81e7-a09343aa597b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {'text': 'मेरा', 'xpos': 'PRP', 'id': 1, 'lemma': 'मैं', 'deprel': 'nmod', 'head': 2}\n",
      "2 {'text': 'भारत', 'xpos': 'NNP', 'id': 2, 'lemma': 'भारत', 'deprel': 'nsubj', 'head': 3}\n",
      "3 {'text': 'महान', 'xpos': 'JJ', 'id': 3, 'lemma': 'महान', 'deprel': 'root', 'head': 0}\n",
      "4 {'text': 'है', 'xpos': 'VM', 'id': 4, 'lemma': 'है', 'deprel': 'cop', 'head': 3}\n"
     ]
    }
   ],
   "source": [
    "for key,value in word_sign_form.items():\n",
    "    print(key,value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8027d0-6fea-4f45-8027-e1132acfc13c",
   "metadata": {},
   "source": [
    "# Handling Adjective and Adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12b3bc49-fd8e-4d95-a55c-2730263f557d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange Adjective and Adverb\n",
    "for key,value in word_tag_cleaned.items():    # word_tag_cleaned\n",
    "    if value['xpos'] in ['JJ']:     # Adjective Adverb\n",
    "        # first condition is for when it does not have corresponding noun or verb - मैं खुश हूं।\n",
    "        if value['head']!=0 and word_tag_cleaned[value['head']]['xpos'] in ['NN']:\n",
    "            word_sign_form = change_dict_order(word_sign_form,key,value['head'])\n",
    "    elif value['xpos'] in ['RB']:\n",
    "        if value['head']!=0 and word_tag_cleaned[value['head']]['xpos'] in ['VM','VAUX']:\n",
    "            word_sign_form = change_dict_order(word_sign_form,key,value['head'])\n",
    "#print(word_sign_form)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d039b79-0f1d-4c9e-b979-ecfcd42fd957",
   "metadata": {},
   "source": [
    "# Handling Negative Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86d0f059-dd69-49d8-9f1a-01a450617fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange Negative Sentences\n",
    "for key,value in word_sign_form.items():\n",
    "    if value['xpos']=='NEG': \n",
    "        last_key = list(word_sign_form.keys())[-1]\n",
    "        word_sign_form = change_dict_order(word_sign_form,key,last_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebd38bf-0d02-4dc4-9efa-b71868de7d83",
   "metadata": {},
   "source": [
    "# Handling Interrogative Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6dce53b-e21c-41b1-84db-bb56a011bcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling Interrogative Sentence\n",
    "for key,value in word_sign_form.items():\n",
    "    if value['xpos']=='WQ': \n",
    "        last_key = list(word_sign_form.keys())[-1]\n",
    "        word_sign_form = change_dict_order(word_sign_form,key,last_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ec4a17-c6ad-4d7b-8197-c207e004e128",
   "metadata": {},
   "source": [
    "# StopWord Removal - For Sentences like मैं खुश हूं।\n",
    "#### हूं - comes out to be VM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d6aa9b6-a5f5-40be-b999-51efa9cf8da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read stopwords from file\n",
    "with open('./utility/final_stopwords.txt', 'r',encoding='utf8') as file:\n",
    "    # Read the entire contents of the file\n",
    "    stopword_list = file.readlines()\n",
    "stopword_list = [word.strip() for word in stopword_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5a534df-a946-479a-be91-951539f7b6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StopWord Removal\n",
    "stopword_removed_list = {}\n",
    "for key,value in word_sign_form.items():\n",
    "    #print(value['text'] in stopword_list)\n",
    "    if value['text'] not in stopword_list:\n",
    "        stopword_removed_list[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd1c67d0-8a61-4bdf-a55f-220fbaa30476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {'text': 'मेरा', 'xpos': 'PRP', 'id': 1, 'lemma': 'मैं', 'deprel': 'nmod', 'head': 2}\n",
      "2 {'text': 'भारत', 'xpos': 'NNP', 'id': 2, 'lemma': 'भारत', 'deprel': 'nsubj', 'head': 3}\n",
      "3 {'text': 'महान', 'xpos': 'JJ', 'id': 3, 'lemma': 'महान', 'deprel': 'root', 'head': 0}\n"
     ]
    }
   ],
   "source": [
    "for key,value in stopword_removed_list.items():\n",
    "    print(key,value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e686ca02-4b16-4d06-b580-3bdc2297492a",
   "metadata": {},
   "source": [
    "# Mapping xpos to POS Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c99404b-f6b5-4efb-9a1c-3ccd3424e552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping xpos to POS tags\n",
    "xpos_to_pos = {\n",
    "    'NNP': 'pnoun',\n",
    "    'VM': 'verb',\n",
    "    'VAUX': 'verb',\n",
    "    'JJ': 'adjective',\n",
    "    'RB': 'adverb',\n",
    "    'PRP' : 'pronoun',\n",
    "    'NEG' : 'negative',\n",
    "    'NN' : 'noun',\n",
    "    'RDP' : 'adverb',\n",
    "    'QF': 'adjective',            # 'अधिक'\\\n",
    "    'WQ': 'wh_adverb',\n",
    "    'NST': 'noun',\n",
    "    'DEM': 'noun_refer_specific',\n",
    "    'INTF': 'intensifier',\n",
    "    # Add more mappings as needed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d4298ba-cb9d-4d2c-95ea-730d5a399596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('मेरा', 'pronoun'), ('भारत', 'pnoun'), ('महान', 'adjective')]\n"
     ]
    }
   ],
   "source": [
    "# Extract Words from Parser and corresponding tag\n",
    "sign_words_list = []\n",
    "for key,value in stopword_removed_list.items():\n",
    "    if value['xpos'] in xpos_to_pos:\n",
    "        sign_words_list.append((value['text'], xpos_to_pos[value['xpos']]))\n",
    "    else:\n",
    "        sign_words_list.append((value['text'], 'extra'))\n",
    "print(sign_words_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b50dac-bc18-44eb-98d4-7aed38eba4f5",
   "metadata": {},
   "source": [
    "# READ ISL Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0236114d-93e8-4a6b-8705-9619b997c142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ ISL Dictionary\n",
    "# Open the file in read mode\n",
    "with open('./utility/isl_dict.txt', 'r',encoding='utf8') as file:\n",
    "    # Read the entire contents of the file\n",
    "    isl_dict = ast.literal_eval(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbf1d943-8d0d-43ad-ad8f-1f4587869c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use Case : Why_(Sign_2) should be why\n",
    "# Remove content within parentheses and strip whitespace for keys containing \"_(*)\"\n",
    "isl_dict = {re.sub(r'_\\(.*\\)', '', key).strip().lower(): value for key, value in isl_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2a794e3-b0b0-444c-87cd-d3f9cf4ef99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dictionary with lowercase keys\n",
    "isl_dict = {key.lower(): value for key, value in isl_dict.items()}\n",
    "isl_dict['school'] = 'स्कूल'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5558836-b586-4fc4-9daf-c7b368cbde42",
   "metadata": {},
   "source": [
    "# Synonym Substitution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1956169d-5b79-4e73-a3fa-48e1d9f24ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-03:19:21:57,282 INFO     [iwn.py:43] Loading hindi language synsets...\n"
     ]
    }
   ],
   "source": [
    "# Synonym Substitution\n",
    "import pyiwn\n",
    "from nltk.corpus import wordnet as wn\n",
    "iwn = pyiwn.IndoWordNet() \n",
    "#print(dir(iwn))\n",
    "#print(iwn.synsets('आम्र'))\n",
    "#print(iwn.all_synsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c95a88d-2051-4eb9-8bde-fd8bed0298aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Translator\n",
    "from googletrans import Translator\n",
    "# Create a Translator object\n",
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76cfa64c-898b-4d10-be2f-8001c0fe10dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling Special Case for Mapping to Videos\n",
    "special_videos = {\n",
    "    'i' : '@D:\\\\desktop\\\\project\\\\Linguistic\\\\I\\\\I_Me.mp4',\n",
    "    'who': '@D:\\\\desktop\\\\project\\\\Linguistic\\\\W\\\\Who_Whom.mp4',\n",
    "    'whom': '@D:\\\\desktop\\\\project\\\\Linguistic\\\\W\\\\Who_Whom.mp4'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62945f12-845e-4762-b1b1-6bfd8a9b698e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('मेरा', 'pronoun', 'my'), ('भारत', 'pnoun', 'india'), ('महान', 'adjective', 'neat')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "lemmatizer = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "synonym_substituted_list = []\n",
    "temp_list = [('आकलन', 'noun')]\n",
    "for word,pos_tag in sign_words_list:\n",
    "\n",
    "    # Translate the Hindi sentence to English\n",
    "    english_word = translator.translate(word, src='hi', dest='en').text.lower()\n",
    "    english_word_lemmatized = lemmatizer(english_word)[0].lemma_.lower()\n",
    "    #print(word,english_word)\n",
    "    \n",
    "    if english_word_lemmatized in list(special_videos.keys()):\n",
    "        synonym_substituted_list.append((word,pos_tag,special_videos[english_word_lemmatized]))\n",
    "        continue\n",
    "    \n",
    "    if pos_tag == 'pnoun':\n",
    "        synonym_substituted_list.append((word,pos_tag,english_word))\n",
    "        continue\n",
    "    #print(word,pos_tag)\n",
    "    \n",
    "    # Case 1 : Check hindi word in isl_dict\n",
    "    if word in list(isl_dict.values()):\n",
    "        synonym_substituted_list.append((word,pos_tag,english_word))\n",
    "        continue\n",
    "    all_hindi_synsets = []\n",
    "    # Case 2 : Check synonym of hindi_word in isl_dict\n",
    "    try:\n",
    "        all_hindi_synsets = iwn.synsets(word)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    flag = False\n",
    "    for synset in all_hindi_synsets:\n",
    "        if synset._head_word in list(isl_dict.values()):\n",
    "            corresponding_keys = [key for key, value in isl_dict.items() if value == synset._head_word]\n",
    "            synonym_substituted_list.append((word,pos_tag,corresponding_keys[0]))\n",
    "            flag = True\n",
    "            break\n",
    "    if flag == True:\n",
    "        continue\n",
    "                \n",
    "    # Case 3 : Check english word in isl_dict\n",
    "    if english_word in list(isl_dict.keys()):\n",
    "        synonym_substituted_list.append((word,pos_tag,english_word))\n",
    "        continue\n",
    "\n",
    "    # Case 4 : Check lemmatized english word in isl_dict\n",
    "    if english_word_lemmatized in list(isl_dict.keys()):\n",
    "        synonym_substituted_list.append((word,pos_tag,english_word_lemmatized))\n",
    "        continue\n",
    "\n",
    "    # Case 5 : Check syno of english word in isl_dict\n",
    "    all_english_synsets = wn.synonyms(english_word)\n",
    "    #print(all_english_synsets)\n",
    "    all_english_synsets_flatten = []\n",
    "    for row in all_english_synsets:\n",
    "        all_english_synsets_flatten.extend(row)\n",
    "    flag = False\n",
    "    for synset in all_english_synsets_flatten:\n",
    "        if synset.lower() in list(isl_dict.keys()):\n",
    "            flag = True\n",
    "            # print('Yes Present')\n",
    "            synonym_substituted_list.append((word,pos_tag,synset.lower()))\n",
    "            break\n",
    "    if flag == True:\n",
    "        continue\n",
    "\n",
    "\n",
    "    # Case 6  : Nothing Words Go for Finger Spelling\n",
    "    synonym_substituted_list.append((word,pos_tag,'#'))\n",
    "print(synonym_substituted_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a4a7964-5c69-4e45-b9c0-56b502243502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hindi Word</th>\n",
       "      <th>POS Tag</th>\n",
       "      <th>ISL Dictionary Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>मेरा</td>\n",
       "      <td>pronoun</td>\n",
       "      <td>my</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>भारत</td>\n",
       "      <td>pnoun</td>\n",
       "      <td>india</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>महान</td>\n",
       "      <td>adjective</td>\n",
       "      <td>neat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Hindi Word    POS Tag ISL Dictionary Tag\n",
       "0       मेरा    pronoun                 my\n",
       "1       भारत      pnoun              india\n",
       "2       महान  adjective               neat"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final ISL List\n",
    "final_isl_list = synonym_substituted_list.copy()\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(final_isl_list, columns=['Hindi Word', 'POS Tag', 'ISL Dictionary Tag'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1570af-2dcc-4bd0-a476-4f2400368ac5",
   "metadata": {},
   "source": [
    "# Video Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f32f543b-b8b2-4e3f-8e83-230f9aea59b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reversed dictionary mapping Hindi words to English words\n",
    "isl_hindi_english_dict = {hindi_word: english_word for english_word, hindi_word in isl_dict.items()}\n",
    "#print(isl_hindi_english_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4dcc232e-7ded-4ef5-874e-d889430f16db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of Devanagari vowel signs to their vowels\n",
    "sign_mapping_vowels = {\n",
    "    'ा': 'आ',  # Aa\n",
    "    'ि': 'इ',  # I\n",
    "    'ी': 'ई',  # II\n",
    "    'ु': 'उ',  # U\n",
    "    'ू': 'ऊ',  # UU\n",
    "    'ृ': 'ऋ',  # R\n",
    "    'े': 'ए',  # E\n",
    "    'ै': 'ऐ',  # AI\n",
    "    'ो': 'ओ',  # O\n",
    "    'ौ': 'औ',  # AU\n",
    "    'ं': 'अं', # Anusvara\n",
    "    'ः': 'अः'  # Visarga\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "16cca5fc-cb58-4d8a-9971-6aa20cb54e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "def search_videos(folder_path, final_isl_list):\n",
    "    \"\"\"\n",
    "    Searches for video files named after the provided words in a directory.\n",
    "\n",
    "    Args:\n",
    "        folder_path: Path to the directory containing video files.\n",
    "        words: A list of words to search for (video names).\n",
    "\n",
    "    Returns:\n",
    "        A list of paths to the found video files.\n",
    "    \"\"\"\n",
    "\n",
    "    found_videos = []\n",
    "    # Assuming `words` is a list of tuples like [(or_word1, pos_tag1, isl_word1), (or_word2, pos_tag2, isl_word2), ...]\n",
    "    for or_word, pos_tag, isl_word in final_isl_list:\n",
    "        video_name = f\"{isl_word.capitalize()}.mp4\"\n",
    "        if pos_tag == 'pnoun':  # Alphabets and FingerSpell\n",
    "            for letter in isl_word:\n",
    "                video_name = f\"{letter.capitalize()}.mp4\"\n",
    "                for root, dirs, files in os.walk(folder_path):\n",
    "                    full_path = os.path.join(root, video_name)\n",
    "                    if os.path.isfile(full_path):\n",
    "                        found_videos.append(full_path)\n",
    "        elif isl_word=='#':\n",
    "            for char in or_word:\n",
    "                if char in sign_mapping_vowels.keys():\n",
    "                    video_name = f\"{sign_mapping_vowels[char]}.mp4\"\n",
    "                else:\n",
    "                    video_name = f\"{char}.mp4\"\n",
    "                for root, dirs, files in os.walk(folder_path):\n",
    "                    full_path = os.path.join(root, video_name)\n",
    "                    if os.path.isfile(full_path):\n",
    "                        found_videos.append(full_path)\n",
    "        elif isl_word[0]=='@':         # Special Words are handled here\n",
    "            found_videos.append(isl_word[1:])\n",
    "        else:\n",
    "            for root, dirs, files in os.walk(folder_path):\n",
    "                full_path = os.path.join(root, video_name)\n",
    "                if os.path.isfile(full_path):\n",
    "                    found_videos.append(full_path)\n",
    "\n",
    "    return found_videos\n",
    "\n",
    "\n",
    "def play_videos(video_paths):\n",
    "    vlc_path = r'C:\\Program Files\\VideoLAN\\VLC\\vlc.exe'  # Path to VLC media player executable\n",
    "    for video_path in video_paths:\n",
    "        subprocess.Popen([vlc_path, '--fullscreen', video_path])\n",
    "        time.sleep(5)  # Delay before playing the next video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79086730-2fda-46d2-bb7a-d10c9357455e",
   "metadata": {},
   "source": [
    "# Set Folder Path where ISL videos are there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f34f5062-592d-4673-8931-c9b9a8a84383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set it accordingly\n",
    "folder_path = 'D:\\projects\\Linguistic-Videos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "72107c58-36dd-4bf2-b396-1b2e4c5b7529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D:\\\\projects\\\\Linguistic-Videos\\\\M\\\\My.mp4', 'D:\\\\projects\\\\Linguistic-Videos\\\\Alphabets\\\\I.mp4', 'D:\\\\projects\\\\Linguistic-Videos\\\\Alphabets\\\\N.mp4', 'D:\\\\projects\\\\Linguistic-Videos\\\\Alphabets\\\\D.mp4', 'D:\\\\projects\\\\Linguistic-Videos\\\\Alphabets\\\\I.mp4', 'D:\\\\projects\\\\Linguistic-Videos\\\\Alphabets\\\\A.mp4', 'D:\\\\projects\\\\Linguistic-Videos\\\\N\\\\Neat.mp4']\n"
     ]
    }
   ],
   "source": [
    "video_paths = search_videos(folder_path, final_isl_list)\n",
    "print(video_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "072f48c5-87c7-46e7-954c-07a78e698a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video merged_video.mp4.\n",
      "MoviePy - Writing audio in merged_videoTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video merged_video.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "must be real number, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m merged_clip \u001b[38;5;241m=\u001b[39m merge_videos(video_paths)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Export the merged video to a file\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[43mmerged_clip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_videofile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmerged_video.mp4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mfps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Play the merged video\u001b[39;00m\n\u001b[0;32m     16\u001b[0m subprocess\u001b[38;5;241m.\u001b[39mPopen([vlc_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--fullscreen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmerged_video.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\Desktop\\Projects\\Hindi_To_IndianSignLanguage\\venv\\Lib\\site-packages\\decorator.py:232\u001b[0m, in \u001b[0;36mfun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m evaldict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(_call_\u001b[38;5;241m=\u001b[39mcaller, _func_\u001b[38;5;241m=\u001b[39mfunc)\n\u001b[0;32m    231\u001b[0m es \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, extra \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(extras):\n\u001b[0;32m    233\u001b[0m     ex \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_e\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m i\n\u001b[0;32m    234\u001b[0m     evaldict[ex] \u001b[38;5;241m=\u001b[39m extra\n",
      "File \u001b[1;32m~\\Desktop\\Projects\\Hindi_To_IndianSignLanguage\\venv\\Lib\\site-packages\\moviepy\\decorators.py:54\u001b[0m, in \u001b[0;36mrequires_duration\u001b[1;34m(f, clip, *a, **k)\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mduration\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not set\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\Projects\\Hindi_To_IndianSignLanguage\\venv\\Lib\\site-packages\\decorator.py:232\u001b[0m, in \u001b[0;36mfun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m evaldict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(_call_\u001b[38;5;241m=\u001b[39mcaller, _func_\u001b[38;5;241m=\u001b[39mfunc)\n\u001b[0;32m    231\u001b[0m es \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, extra \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(extras):\n\u001b[0;32m    233\u001b[0m     ex \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_e\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m i\n\u001b[0;32m    234\u001b[0m     evaldict[ex] \u001b[38;5;241m=\u001b[39m extra\n",
      "File \u001b[1;32m~\\Desktop\\Projects\\Hindi_To_IndianSignLanguage\\venv\\Lib\\site-packages\\moviepy\\decorators.py:135\u001b[0m, in \u001b[0;36muse_clip_fps_by_default\u001b[1;34m(f, clip, *a, **k)\u001b[0m\n\u001b[0;32m    130\u001b[0m new_a \u001b[38;5;241m=\u001b[39m [fun(arg) \u001b[38;5;28;01mif\u001b[39;00m (name\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfps\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m arg\n\u001b[0;32m    131\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m (arg, name) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(a, names)]\n\u001b[0;32m    132\u001b[0m new_kw \u001b[38;5;241m=\u001b[39m {k: fun(v) \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfps\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m v\n\u001b[0;32m    133\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m (k,v) \u001b[38;5;129;01min\u001b[39;00m k\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m--> 135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_kw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\Projects\\Hindi_To_IndianSignLanguage\\venv\\Lib\\site-packages\\decorator.py:232\u001b[0m, in \u001b[0;36mfun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m evaldict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(_call_\u001b[38;5;241m=\u001b[39mcaller, _func_\u001b[38;5;241m=\u001b[39mfunc)\n\u001b[0;32m    231\u001b[0m es \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, extra \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(extras):\n\u001b[0;32m    233\u001b[0m     ex \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_e\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m i\n\u001b[0;32m    234\u001b[0m     evaldict[ex] \u001b[38;5;241m=\u001b[39m extra\n",
      "File \u001b[1;32m~\\Desktop\\Projects\\Hindi_To_IndianSignLanguage\\venv\\Lib\\site-packages\\moviepy\\decorators.py:22\u001b[0m, in \u001b[0;36mconvert_masks_to_RGB\u001b[1;34m(f, clip, *a, **k)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clip\u001b[38;5;241m.\u001b[39mismask:\n\u001b[0;32m     21\u001b[0m     clip \u001b[38;5;241m=\u001b[39m clip\u001b[38;5;241m.\u001b[39mto_RGB()\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\Projects\\Hindi_To_IndianSignLanguage\\venv\\Lib\\site-packages\\moviepy\\video\\VideoClip.py:300\u001b[0m, in \u001b[0;36mVideoClip.write_videofile\u001b[1;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, logger)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m make_audio:\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio\u001b[38;5;241m.\u001b[39mwrite_audiofile(audiofile, audio_fps,\n\u001b[0;32m    294\u001b[0m                                audio_nbytes, audio_bufsize,\n\u001b[0;32m    295\u001b[0m                                audio_codec, bitrate\u001b[38;5;241m=\u001b[39maudio_bitrate,\n\u001b[0;32m    296\u001b[0m                                write_logfile\u001b[38;5;241m=\u001b[39mwrite_logfile,\n\u001b[0;32m    297\u001b[0m                                verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    298\u001b[0m                                logger\u001b[38;5;241m=\u001b[39mlogger)\n\u001b[1;32m--> 300\u001b[0m \u001b[43mffmpeg_write_video\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mbitrate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbitrate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mpreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mwrite_logfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_logfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m                   \u001b[49m\u001b[43maudiofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudiofile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mffmpeg_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mffmpeg_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remove_temp \u001b[38;5;129;01mand\u001b[39;00m make_audio:\n\u001b[0;32m    310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(audiofile):\n",
      "File \u001b[1;32m~\\Desktop\\Projects\\Hindi_To_IndianSignLanguage\\venv\\Lib\\site-packages\\moviepy\\video\\io\\ffmpeg_writer.py:213\u001b[0m, in \u001b[0;36mffmpeg_write_video\u001b[1;34m(clip, filename, fps, codec, bitrate, preset, withmask, write_logfile, audiofile, verbose, threads, ffmpeg_params, logger)\u001b[0m\n\u001b[0;32m    211\u001b[0m     logfile \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    212\u001b[0m logger(message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMoviepy - Writing video \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m filename)\n\u001b[1;32m--> 213\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mFFMPEG_VideoWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodec\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcodec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mpreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbitrate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbitrate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m                            \u001b[49m\u001b[43maudiofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudiofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mffmpeg_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mffmpeg_params\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m writer:\n\u001b[0;32m    218\u001b[0m     nframes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(clip\u001b[38;5;241m.\u001b[39mduration\u001b[38;5;241m*\u001b[39mfps)\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t,frame \u001b[38;5;129;01min\u001b[39;00m clip\u001b[38;5;241m.\u001b[39miter_frames(logger\u001b[38;5;241m=\u001b[39mlogger, with_times\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    221\u001b[0m                                     fps\u001b[38;5;241m=\u001b[39mfps, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muint8\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\Desktop\\Projects\\Hindi_To_IndianSignLanguage\\venv\\Lib\\site-packages\\moviepy\\video\\io\\ffmpeg_writer.py:88\u001b[0m, in \u001b[0;36mFFMPEG_VideoWriter.__init__\u001b[1;34m(self, filename, size, fps, codec, audiofile, preset, bitrate, withmask, logfile, threads, ffmpeg_params)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mext \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# order is important\u001b[39;00m\n\u001b[0;32m     80\u001b[0m cmd \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     81\u001b[0m     get_setting(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFFMPEG_BINARY\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-y\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-loglevel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logfile \u001b[38;5;241m==\u001b[39m sp\u001b[38;5;241m.\u001b[39mPIPE \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfo\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-f\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrawvideo\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-vcodec\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrawvideo\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-s\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124mx\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (size[\u001b[38;5;241m0\u001b[39m], size[\u001b[38;5;241m1\u001b[39m]),\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-pix_fmt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrgba\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m withmask \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrgb24\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-r\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m%.02f\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfps\u001b[49m,\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-an\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-i\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     90\u001b[0m ]\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m audiofile \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     cmd\u001b[38;5;241m.\u001b[39mextend([\n\u001b[0;32m     93\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-i\u001b[39m\u001b[38;5;124m'\u001b[39m, audiofile,\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-acodec\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcopy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     95\u001b[0m     ])\n",
      "\u001b[1;31mTypeError\u001b[0m: must be real number, not NoneType"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip, concatenate_videoclips\n",
    "vlc_path = r'C:\\Program Files\\VideoLAN\\VLC\\vlc.exe' \n",
    "def merge_videos(video_paths):\n",
    "    clips = [VideoFileClip(path) for path in video_paths]\n",
    "    final_clip = concatenate_videoclips(clips, method=\"compose\")\n",
    "    return final_clip\n",
    "\n",
    "# Merge the videos into a single video\n",
    "merged_clip = merge_videos(video_paths)\n",
    "\n",
    "\n",
    "# Export the merged video to a file\n",
    "merged_clip.write_videofile(\"merged_video.mp4\",fps=24)\n",
    "\n",
    "# Play the merged video\n",
    "subprocess.Popen([vlc_path, '--fullscreen', 'merged_video.mp4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad233a8-4ba9-485f-b44b-f4601cfdda21",
   "metadata": {},
   "source": [
    "# Hope You Like the project. :D"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
